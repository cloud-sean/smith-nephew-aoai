{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a client for the chroma vector store locally\n",
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "chroma_client.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, openai\n",
    "os.environ[\"OPENAI_API_TYPE\"] = openai.api_type = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = openai.api_version = \"2022-12-01\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = openai.api_base = \"https://openai-endpoint.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key = \"ENTER YOUR API KEY HERE\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will be used to embed documents when they are entered into the vector store\n",
    "from chromadb.utils import embedding_functions\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "                model_name=\"text-embedding-ada-002\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to use the directory loader which will leverage a local version of Unstructured-io\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader('data/', glob=\"**/*.pdf\")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all the documents that were loaded\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split every document into chunks \n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:12<00:00,  4.78it/s]\n"
     ]
    }
   ],
   "source": [
    "#generate embeddings for each chunk\n",
    "import time, tqdm\n",
    "embeddings = []\n",
    "\n",
    "for text in tqdm.tqdm(texts):\n",
    "    try:\n",
    "        response = openai.Embedding.create(\n",
    "            input=text.page_content,\n",
    "            engine=\"text-embedding-ada-002\")\n",
    "        emb = response['data'][0]['embedding']\n",
    "        embeddings.append(emb)\n",
    "    except Exception as e:\n",
    "        time.sleep(8)\n",
    "        response = openai.Embedding.create(\n",
    "            input=text.page_content,\n",
    "            engine=\"text-embedding-ada-002\")\n",
    "        emb = response['data'][0]['embedding']\n",
    "        embeddings.append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# initialize the vector store and create a openai embedding function\n",
    "azure_embeddings = OpenAIEmbeddings(document_model_name=\"text-embedding-ada-002\",query_model_name=\"text-embedding-ada-002\")\n",
    "vectorstore = Chroma(\"my_collection\", embedding_function=azure_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the documents to the vector store\n",
    "vectorstore._collection.add(\n",
    "    ids= [f\"doc_{i}\" for i in range(len(texts))],\n",
    "    documents=[texts[i].page_content for i in range(len(texts))],\n",
    "    embeddings=embeddings,\n",
    "    metadatas=[text.metadata for text in texts]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can create custom templates pass them in later\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. If there are multiple answers, describe them all.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    ":\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import VectorDBQAWithSourcesChain\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "#initialize our chain\n",
    "# THIS IS WHERE YOU CAN PASS YOUR PROMPT TEMPLATE !\n",
    "chain = VectorDBQAWithSourcesChain.from_chain_type(llm=AzureOpenAI(deployment_name=\"davinci003\", model_name=\"text-davinci-003\"), chain_type=\"stuff\", vectorstore=vectorstore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': ' The overall prevalence of venous leg ulcers (VLUs) in the Western population is 1%, rising to 3% in those over 65 years of age.\\n', 'sources': 'data/35736 PICO for venous leg ulcers (VLUs) OUS RoW PDF 0522.pdf'}\n"
     ]
    }
   ],
   "source": [
    "# test the chain\n",
    "answer = chain({\"question\": \"What is the incidence rate of venous leg ulcers (VLUs) in the Western population?\"}, return_only_outputs=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_answer(question):\n",
    "    answer = chain({'question': question}, return_only_outputs=True)\n",
    "    answer = f\"\"\"{answer['answer']}\\n\\n source: {answer['sources']}\"\"\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr \n",
    "with gr.Blocks() as demo:\n",
    "    text = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n",
    "    answerbox = gr.Textbox(show_label=False, placeholder=\"Answer\").style(container=False)\n",
    "    text.submit(return_answer, inputs=[text], outputs=[answerbox])\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54cb3a929f2c087f813f65eaef6c4237043491f4fb056630746c8f68ea06fe11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
